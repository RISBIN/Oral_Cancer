# CHAPTER 2: SYSTEM ANALYSIS

## 2.1 FEASIBILITY STUDY

The feasibility study represents a critical phase in the OralCare AI project lifecycle, providing systematic evaluation of whether the proposed system is viable from technical, economic, operational, and schedule perspectives before committing substantial resources to full-scale development. This comprehensive analysis examines multiple dimensions of feasibility to ensure that the project can realistically achieve its objectives within the constraints of available resources, technology capabilities, organizational readiness, and market conditions. The feasibility study serves not only as a go/no-go decision point but also as a planning tool that identifies potential challenges, resource requirements, and risk mitigation strategies that will shape the implementation approach. By conducting rigorous feasibility analysis upfront, the project team can make informed decisions about scope, approach, and resource allocation while setting realistic expectations with stakeholders about what the system can deliver and within what timeframe.

### Technical Feasibility

Technical feasibility examines whether the proposed OralCare AI system can be successfully built using available technology, tools, and expertise, considering both the state-of-the-art in relevant domains and the specific capabilities of the development team. From a machine learning perspective, the technical feasibility is strongly supported by substantial research demonstrating that convolutional neural networks can effectively classify medical images including oral lesions, with published studies showing that both RegNetY320 and VGG16 architectures achieve high accuracy on oral cancer detection tasks when trained on appropriate datasets. The availability of these architectures through open-source deep learning frameworks like TensorFlow and PyTorch, along with pre-trained model weights that can be fine-tuned on medical imaging datasets, significantly reduces the technical risk compared to developing novel architectures from scratch. The development team's access to GPU-accelerated computing infrastructure through cloud platforms like AWS, Google Cloud, or Azure provides the computational resources necessary for model training and optimization, while the relatively modest inference requirements of the final models enable deployment on standard cloud instances without specialized hardware.

The web application development aspects of the project demonstrate strong technical feasibility through the use of mature, well-documented technology stacks that have been proven in countless production deployments. Django, the chosen web framework, provides a robust foundation with built-in security features, ORM for database abstraction, authentication systems, and extensive ecosystem of third-party packages addressing common requirements. The framework's "batteries included" philosophy means that many features required for OralCare AI—including user authentication, database migrations, form handling, and admin interfaces—are available out-of-the-box with minimal custom development required. The frontend technology stack leveraging HTML5, CSS3, JavaScript, and Bootstrap enables creation of responsive, accessible user interfaces that work across devices and browsers, while libraries like jQuery simplify common tasks like AJAX requests and DOM manipulation. The integration of Django with Supabase for database and file storage is well-supported through PostgreSQL drivers and HTTP APIs, with extensive documentation and community resources available to address integration challenges.

The image processing pipeline presents moderate technical complexity that is well within feasibility boundaries given available tools and expertise. Python's extensive ecosystem of image processing libraries including Pillow for basic operations, OpenCV for advanced computer vision tasks, and NumPy for numerical computations provides all necessary capabilities for preprocessing medical images prior to model inference. Common preprocessing steps required for the project—including resizing images to match model input dimensions, normalizing pixel values to expected ranges, converting color spaces when necessary, and handling various image formats—are straightforward operations well-documented in library references and tutorials. The cloud storage integration for medical images can leverage Supabase's storage APIs which provide simple HTTP-based interfaces for uploading, retrieving, and managing files with appropriate access controls and encryption, avoiding the complexity of managing file systems and backups manually.

Security and compliance requirements, while adding complexity, are technically achievable through well-established practices and tools. HTTPS encryption for all data transmission can be implemented through SSL/TLS certificates obtained from free providers like Let's Encrypt and configured in web server settings. User authentication and authorization can leverage Django's built-in systems which have been extensively tested and hardened against common vulnerabilities like SQL injection, cross-site scripting, and cross-site request forgery. Database encryption at rest can be enabled through Supabase's platform features which encrypt data using industry-standard algorithms transparent to the application. Audit logging capturing user actions and system events can be implemented through Django middleware and custom logging handlers that record activities to database tables or log aggregation services. The team's access to security expertise through Adyog's cybersecurity practice provides additional assurance that security requirements can be properly addressed.

The primary technical risks identified include potential challenges in achieving target model accuracy on real-world clinical images that may differ in quality and characteristics from training datasets, inference latency that could make the system frustratingly slow for clinical workflows if not properly optimized, and integration complexities between various system components that could emerge during development. However, these risks can be mitigated through strategies including careful dataset curation with images representative of actual clinical conditions, model optimization techniques like quantization and pruning to improve inference speed, and iterative integration testing to identify and resolve component compatibility issues early. Overall, the technical feasibility assessment concludes that the OralCare AI system can be successfully built using available technology and expertise, with manageable risks that can be addressed through standard software development practices.

### Economic Feasibility

Economic feasibility evaluates whether the OralCare AI project represents a sound financial investment by analyzing development costs, ongoing operational expenses, and potential benefits in both monetary and non-monetary terms. From a cost perspective, the project benefits from several factors that keep development expenses reasonable relative to the value created. The use of open-source frameworks and libraries for both machine learning and web development eliminates licensing costs that would be incurred with proprietary alternatives, while cloud-based infrastructure operating on pay-as-you-go pricing models avoids large upfront capital expenditures for servers and data center facilities. The development team structure, primarily consisting of skilled engineers from Adyog working as part of a college project, represents efficient resource utilization compared to hiring specialized external consultants or research scientists. The availability of pre-trained model weights that can be fine-tuned rather than training from scratch significantly reduces the computational costs and time required for model development.

Detailed cost estimation for the project breaks down into several categories that together define the total economic investment required. Development labor costs represent the largest expense category, encompassing software engineers for web application development, data scientists for model training and optimization, UI/UX designers for interface design, and project managers for coordination and stakeholder management. Cloud infrastructure costs during development include compute instances for model training with GPU acceleration, database hosting for development and testing environments, file storage for image datasets and system backups, and bandwidth for data transfer during development and testing. Third-party services and tools include development tools and IDEs, version control and collaboration platforms, testing and quality assurance tools, and monitoring and analytics services. Training and documentation costs cover creation of user manuals and system documentation, training materials for healthcare providers, and potentially video tutorials demonstrating system usage.

Ongoing operational costs that will be incurred after initial deployment include cloud infrastructure for production systems, scaling with user adoption and data volume growth. Database and storage costs increase as more images are uploaded and stored, though optimization strategies like compression and archival of old data can help manage growth. Bandwidth and data transfer costs scale with system usage, particularly for image uploads and report downloads. Maintenance and support costs include bug fixes and security patches, user support and helpdesk operations, periodic model retraining with new data, and feature enhancements based on user feedback. Compliance and security costs cover security audits and penetration testing, compliance certifications if required, insurance for liability protection, and legal counsel for regulatory matters.

The benefits of the OralCare AI system, while more difficult to quantify precisely than costs, are substantial and multi-dimensional. Direct financial benefits to healthcare systems include reduced costs from avoiding late-stage cancer treatment, which is dramatically more expensive than early-stage intervention, reduced unnecessary biopsies of benign lesions through more accurate screening, and operational efficiency gains from rapid screening compared to manual specialist review. Indirect economic benefits encompass broader societal impact including productivity gains from earlier cancer detection enabling patients to maintain work capacity, reduced healthcare system burden from preventing advanced cancer cases, and economic development in medtech sector from successful AI deployment. Non-monetary benefits that are equally important include improved health outcomes through earlier detection and treatment, reduced patient suffering from avoiding late-stage disease, increased access to screening in underserved areas, and advancement of medical AI field through practical deployment example.

Cost-benefit analysis comparing the estimated total investment of development and operational costs against quantified benefits where possible and qualitative benefits where quantification is difficult suggests favorable economics for the project. Even conservative estimates of lives saved or cancers detected earlier through improved screening, multiplied by accepted values of statistical life and quality-adjusted life years, generate benefit values that substantially exceed development costs. The relatively low marginal cost of serving additional users once the system is developed—limited primarily to cloud infrastructure scaling costs—means that benefits grow proportionally with adoption while costs grow sub-linearly, improving the cost-benefit ratio as utilization increases. The educational and research benefits, while difficult to quantify monetarily, represent additional value creation that further strengthens the economic case.

Alternative funding and deployment models can be considered to address economic feasibility from different perspectives. Grant funding from public health organizations, cancer research foundations, or medical AI research programs could offset development costs while aligning with mission-driven objectives. Partnership with healthcare institutions or dental associations could provide both funding and clinical validation opportunities, potentially with revenue sharing from eventual commercial deployment. Freemium model offering basic screening capabilities free to resource-limited settings while charging premium fees to well-resourced institutions could balance access and sustainability. Open-source release of core technology while monetizing support, training, and hosted services could leverage community contributions while creating sustainable business model. Overall, the economic feasibility assessment concludes that the project represents a sound investment with manageable costs, substantial benefits, and multiple viable paths to sustainability.

### Operational Feasibility

Operational feasibility examines whether the proposed OralCare AI system will be successfully adopted and effectively utilized by its intended users within their actual working environments, considering factors including user acceptance, organizational readiness, workflow integration, and change management requirements. This dimension of feasibility is particularly critical for medical AI systems, as even technically excellent solutions can fail if they do not fit naturally into clinical workflows, gain trust from healthcare providers, or demonstrate clear value that justifies the effort of adoption. The operational feasibility analysis must consider the system from multiple user perspectives including clinicians who will perform screenings, administrators who will manage the platform, patients who will be affected by screening results, and healthcare organizations that will deploy and support the system.

From the clinician user perspective, operational feasibility is enhanced by several design choices that minimize friction and learning curves. The web-based interface accessible through standard browsers eliminates installation and configuration burdens that would be required for desktop applications, enabling providers to access the system from any computer with internet connectivity without IT department involvement. The drag-and-drop image upload interface mimics familiar file management operations that most computer users already understand, reducing the learning curve for basic system operation. The rapid analysis turnaround time, with results available within seconds of upload, fits naturally into clinical workflows where providers need timely information to make decisions during or immediately after patient encounters. The PDF report generation feature provides professional output suitable for medical records and patient communication, addressing the documentation requirements that are integral to medical practice. The dual-model comparison showing both RegNetY320 and VGG16 predictions gives providers additional confidence through consensus while flagging cases where models disagree that may warrant particular attention.

The multi-role access system supporting administrators, doctors, researchers, and students addresses diverse stakeholder needs within healthcare and educational organizations. Administrators can manage user accounts, monitor system usage, and access analytics without requiring technical expertise in web development or database management. Doctors can focus on clinical functionality—uploading images, reviewing results, generating reports—without being overwhelmed by administrative or research features not relevant to patient care. Researchers can access aggregate data and analytics enabling epidemiological studies and model performance analysis without compromising individual patient privacy. Students can use the system as a learning tool with appropriate access restrictions and supervision, supporting educational objectives without interfering with clinical operations. This role-based design recognizes that different users have different needs and skill levels, tailoring the interface and available functionality accordingly.

Workflow integration represents a critical success factor for operational feasibility, as healthcare providers will only consistently use the system if it enhances rather than disrupts their established practice patterns. The screening workflow supported by OralCare AI aligns naturally with existing oral examination procedures where providers visually inspect the oral cavity during routine dental visits or medical checkups. When a suspicious lesion is identified, the provider can capture an image using an intraoral camera or high-resolution smartphone camera, upload it to the OralCare AI platform, and receive immediate AI-assisted assessment informing the decision about whether to refer to a specialist, perform immediate biopsy, or schedule follow-up observation. This workflow introduces minimal additional burden beyond image capture, which many providers already perform for documentation purposes, while providing decision support that can increase confidence in clinical judgments. The system operates asynchronously, meaning providers can upload images at their convenience rather than requiring real-time specialist consultation, accommodating busy clinical schedules.

Change management and training requirements for successful deployment have been carefully considered in the operational feasibility analysis. The initial training required for providers to effectively use the basic system functionality is modest, potentially achievable through a combination of brief instructional videos demonstrating key workflows, written quick-start guides with screenshots and step-by-step instructions, and hands-on practice with test cases to build familiarity and confidence. More advanced features like batch processing or research analytics may require additional training for users who need those capabilities. The intuitive interface design minimizes the ongoing support burden, though helpdesk resources for addressing questions and troubleshooting technical issues will be necessary, potentially provided through email support, FAQ documentation, and potentially live chat or phone support for urgent issues. Champions within adopting organizations—enthusiastic early users who become local experts and advocates—can provide peer support and drive adoption through demonstration of value to colleagues.

User acceptance factors critical to operational success include trust in AI recommendations, which will be built through transparency about model capabilities and limitations, demonstration of accuracy through validation studies, and emphasis that AI serves as decision support rather than autonomous diagnosis. Perceived usefulness and ease of use, both key determinants of technology adoption according to established models like the Technology Acceptance Model, are addressed through the clinical value proposition of improved detection accuracy and the intuitive interface design respectively. Professional and regulatory acceptance within medical and dental communities will require validation studies demonstrating clinical utility, potentially publications in peer-reviewed journals, and endorsement from professional societies or thought leaders in oral pathology and oncology. Patient acceptance and comfort with AI-assisted screening will benefit from provider communication emphasizing that human clinicians make final decisions and that AI serves to enhance rather than replace clinical expertise.

Organizational readiness assessments for potential adopting institutions should evaluate factors including technical infrastructure including reliable internet connectivity and availability of image capture equipment, organizational culture and openness to innovation particularly regarding adoption of new technologies, leadership support for AI-assisted screening initiatives, and resource availability for initial deployment and ongoing operation. Institutions with strong readiness across these dimensions are ideal early adoption targets where successful deployment can create case studies and reference examples encouraging broader diffusion. The operational feasibility assessment concludes that with appropriate attention to workflow integration, training, and change management, the OralCare AI system can achieve successful adoption and utilization delivering meaningful clinical value.

### Schedule Feasibility

Schedule feasibility evaluates whether the OralCare AI project can be completed within acceptable timeframes given available resources, dependencies between tasks, and stakeholder expectations for system availability. The analysis considers the full project lifecycle from initial planning and requirements gathering through development, testing, deployment, and post-launch stabilization, identifying the critical path of activities that determine minimum project duration and potential acceleration opportunities that could shorten time to delivery. For an academic project context where the system must be completed within a specific semester or academic year timeline, schedule feasibility is particularly critical as hard deadlines cannot be easily extended regardless of technical challenges encountered during implementation.

The project timeline can be structured into several sequential phases, each with specific deliverables and duration estimates based on scope and team capacity. The planning and requirements phase, typically lasting two to three weeks, encompasses activities including stakeholder interviews to understand user needs and clinical workflows, literature review of existing oral cancer detection research and AI approaches, technical architecture design defining system components and integration patterns, and project planning establishing detailed task breakdown, resource assignments, and milestone schedule. This phase is critical for ensuring common understanding among team members and stakeholders about project scope and approach, reducing rework later from misaligned expectations. The requirements documentation produced includes functional specifications describing what the system must do, non-functional requirements addressing performance, security, and usability criteria, technical specifications defining architecture and technology choices, and project plan with detailed schedule and resource allocation.

The development phase, consuming the largest portion of project duration at eight to twelve weeks, proceeds through iterative sprints each delivering incremental functionality. Early sprints focus on foundation including database schema implementation and migrations, authentication and user management system, basic UI framework and navigation structure, and cloud infrastructure setup with development and staging environments. Middle sprints implement core functionality including image upload and storage integration with Supabase, model integration and inference pipeline enabling RegNetY320 and VGG16 predictions, results display and comparison interface, and detection history and tracking features. Later sprints add value-added features including PDF report generation with customizable templates, admin dashboard and analytics, user profile and settings management, and UI polish and responsive design refinement. The iterative approach enables early validation of technical approaches and gathering of stakeholder feedback that can inform later development, reducing risk of building the wrong thing.

The testing phase, overlapping with development in modern agile approaches but intensifying toward the end, spans three to four weeks and encompasses multiple testing types ensuring system quality. Unit testing validates individual components in isolation, catching defects early when they are easiest to fix. Integration testing verifies that components work correctly together, identifying interface mismatches and data flow issues. System testing evaluates the complete integrated system against requirements, confirming that all specified functionality works as intended. Performance testing measures response times, throughput, and resource utilization under various load conditions, identifying bottlenecks that could degrade user experience. Security testing including vulnerability scanning and penetration testing identifies potential security weaknesses before they can be exploited. User acceptance testing with representative users performing realistic tasks provides validation that the system meets actual needs and is usable in practice. The testing phase produces test reports documenting defects found and resolved, performance benchmarks demonstrating system capabilities under load, security assessment confirming acceptable risk posture, and user feedback informing final refinements.

The deployment and stabilization phase, lasting one to two weeks, transitions the system from development environment to production operation. Deployment activities include production environment provisioning with appropriate sizing and security hardening, database migration and initial data loading, SSL certificate installation and HTTPS configuration, DNS configuration and domain registration, and smoke testing in production environment verifying basic functionality. User onboarding includes account creation for initial users, training sessions for administrators and early users, documentation distribution including user guides and FAQs, and support channel establishment for addressing user questions. Post-launch monitoring watches for issues including error rates and system stability, performance metrics and resource utilization, user activity and adoption patterns, and feedback collection for improvement priorities. The initial post-launch period typically reveals issues not caught during testing, requiring rapid response and hotfixes to maintain user confidence.

Critical path analysis identifies the sequence of dependent tasks that determine minimum project duration, highlighting activities where delays would directly impact the overall schedule. In the OralCare AI project, the critical path likely runs through model training and optimization, as this work cannot begin until suitable datasets are obtained and prepared, and subsequent development of inference pipeline depends on having trained models available. Other critical path activities include database schema design which must be completed before application development can fully proceed, and core authentication and user management which many other features depend upon. Non-critical path activities that could potentially be parallelized include UI design which can proceed independently while backend development continues, documentation creation which can occur alongside development, and test case development which can be written before implementation is complete.

Schedule risk factors that could cause delays include dataset acquisition challenges if suitable training data proves difficult to obtain or requires extensive cleaning and labeling, model performance issues if initial training runs do not achieve target accuracy requiring architecture changes or hyperparameter tuning, integration complexities between components requiring more time than estimated, scope creep from stakeholder requests for additional features beyond initial requirements, and resource unavailability from team members having competing commitments or unexpected absences. Mitigation strategies include proactive dataset sourcing identifying multiple potential data sources early, buffer time in schedule accounting for uncertainty in estimates, ruthless scope management deferring non-essential features to future versions, and cross-training team members to reduce single-person dependencies.

Schedule optimization opportunities that could potentially accelerate delivery include parallel development of independent components by multiple team members, reuse of existing code and components from previous projects or open-source libraries, reduction of scope by deferring lower-priority features to post-launch releases, and increased resource allocation adding team members to parallelizable tasks, though this faces diminishing returns due to coordination overhead. The schedule feasibility assessment concludes that with realistic estimates, proactive risk management, and disciplined scope control, the OralCare AI project can be completed within typical academic project timelines of one semester or academic year, delivering a functional system with core features while potentially deferring some advanced capabilities to future enhancements.

## 2.2 EXISTING SYSTEM

The existing system for oral cancer detection, which the OralCare AI project aims to augment and improve, consists primarily of traditional clinical examination approaches that have remained largely unchanged for decades despite their well-documented limitations. Understanding the current state of practice is essential for appreciating the value proposition of the proposed AI-assisted system and for ensuring that the new system addresses real needs rather than imaginary problems. The existing system encompasses both the clinical workflows through which oral lesions are identified and diagnosed, and the organizational and systemic factors that influence screening effectiveness and patient outcomes. This analysis examines the existing system from multiple perspectives including clinical methodology, workflow characteristics, technology utilization, organizational infrastructure, and limitations that create opportunities for improvement through AI augmentation.

### Clinical Methodology of Existing System

The foundation of existing oral cancer detection methodology rests on visual and tactile examination of the oral cavity performed by dentists during routine dental checkups or by physicians during general medical examinations. The clinical examination typically begins with extraoral inspection observing the patient's face, neck, and lymph nodes for visible abnormalities or palpable masses that might indicate advanced disease or metastatic spread. Intraoral examination systematically inspects all oral cavity structures including the lips, buccal mucosa, gingiva, hard and soft palate, tongue, floor of mouth, and oropharynx, looking for lesions with concerning characteristics. Clinicians are trained to recognize warning signs including persistent ulcers that do not heal within two weeks, red patches (erythroplakia) or white patches (leukoplakia) that cannot be scraped off, irregular or nodular surface texture, indurated (hardened) lesions suggesting deep infiltration, bleeding without trauma, and asymmetry or mass effect distorting normal anatomy.

When a suspicious lesion is identified through visual examination, the standard diagnostic pathway proceeds through several steps aimed at obtaining definitive histopathological diagnosis. If clinical suspicion is moderate to high, the clinician typically refers the patient to an oral surgeon or ENT specialist for biopsy, which involves removing a small tissue sample from the lesion for microscopic examination by a pathologist. The biopsy procedure, while generally safe, is invasive and carries risks including bleeding, infection, and patient discomfort, along with costs that may be prohibitive in resource-limited settings. The tissue sample is processed in a pathology laboratory where it is fixed, sectioned, stained, and examined under microscopy to identify cellular characteristics indicative of malignancy including dysplasia, abnormal mitotic figures, loss of cellular organization, and invasion beyond the basement membrane. The pathology report classifies the lesion as benign, potentially malignant (dysplasia), or malignant (carcinoma), often with additional grading and staging information that guides treatment planning.

In cases where clinical suspicion is lower or the lesion appearance is ambiguous, clinicians may employ adjunctive diagnostic aids to help distinguish between benign and concerning lesions. Toluidine blue staining involves applying a dye that preferentially binds to dysplastic and malignant cells, causing suspicious areas to stain dark blue while normal tissue remains unstained or lightly stained. Autofluorescence examination uses special lighting that causes normal tissue to fluoresce green while abnormal tissue appears dark due to altered cellular structure and biochemistry. Salivary diagnostics analyze saliva samples for biomarkers associated with oral cancer including proteins, DNA methylation patterns, and microRNA profiles. Brush biopsy, a less invasive alternative to surgical biopsy, uses a small brush to collect cells from the lesion surface for cytological examination, though this technique has lower sensitivity than surgical biopsy and often requires follow-up with traditional biopsy anyway.

The clinical decision-making process in the existing system relies heavily on the individual clinician's knowledge, experience, and judgment, introducing substantial subjectivity and variability. Experienced oral pathologists and oncologists who have examined thousands of lesions develop pattern recognition capabilities that enable accurate diagnosis of subtle cases, but general dentists and primary care physicians may see relatively few oral cancers in their careers, limiting their diagnostic expertise. This expertise gap manifests in both false negatives where concerning lesions are dismissed as benign variations, and false positives where benign lesions generate unnecessary biopsies and patient anxiety. Studies have documented significant inter-observer variability in visual examination findings, with different clinicians reaching different conclusions about the same lesion, and even the same clinician sometimes giving inconsistent assessments when examining the same lesion on different occasions.

### Workflow and Organizational Characteristics

The workflow of the existing system follows a pattern that spans multiple encounters and often weeks or months from initial detection to definitive diagnosis and treatment initiation. The typical patient journey begins with a dental checkup or medical visit where visual examination identifies a lesion, though many patients actually present with self-discovered lesions or symptoms like persistent pain or difficulty swallowing that prompt them to seek care. If the dentist or physician identifies concerning features, they provide a referral to a specialist, which may require the patient to schedule an appointment separately, potentially weeks in the future depending on specialist availability. The specialist appointment involves repeat examination and typically biopsy if the specialist agrees the lesion is suspicious, adding another encounter and procedure. The biopsy tissue is sent to pathology laboratory where processing and examination takes several days to a week, after which results are communicated to the specialist who must then contact the patient to discuss findings. If cancer is diagnosed, additional staging workup including imaging studies and potentially additional biopsies is performed to determine disease extent before treatment planning can begin.

This multi-step, multi-provider workflow creates numerous opportunities for delay and patient dropout from the diagnostic process. Patients may fail to keep specialist appointments due to financial barriers, transportation challenges, fear and anxiety, or competing life demands. Referrals can be lost or delayed in administrative processes between the referring provider and specialist office. Communication gaps between providers can result in important clinical information not being transmitted, requiring redundant examinations. Each transition point in the workflow represents potential friction that slows the diagnostic process and increases the risk of late-stage diagnosis as the disease progresses undetected or unconfirmed.

The organizational infrastructure supporting oral cancer detection in the existing system varies dramatically between high-resource and low-resource settings. Well-resourced healthcare systems in developed countries typically have adequate numbers of dental professionals, oral surgeons, and pathology services, though even in these settings access may be concentrated in urban areas leaving rural populations underserved. Developing countries and rural areas face severe shortages of specialists, with some regions having no oral pathologists or oncologists within hundreds of kilometers, making specialist referral impractical for many patients. The existing system's reliance on specialist expertise for definitive diagnosis creates a bottleneck that limits screening scale, as the rate at which cases can be diagnosed is constrained by specialist capacity rather than by the number of patients who could benefit from screening.

Documentation and information systems in the existing workflow are often paper-based or use basic electronic health records that do not specifically support oral cancer screening workflows. Lesion images, when captured at all, may be stored in disparate systems or paper files that make longitudinal tracking difficult. Lack of standardized reporting formats means that clinical descriptions vary in completeness and detail, potentially omitting information important for diagnosis or treatment planning. The absence of centralized registries or databases aggregating oral cancer cases limits epidemiological surveillance and research into screening effectiveness and treatment outcomes.

### Technology Utilization in Existing System

While the core methodology of visual examination has remained relatively constant, incremental technological improvements have enhanced some aspects of the existing system. Intraoral cameras, which are essentially dental-specific digital cameras with lighting and magnification capabilities, enable high-quality imaging of oral lesions for documentation, patient education, and consultation with specialists via telemedicine. These devices have become relatively common in dental practices in developed countries, though they remain rare in resource-limited settings due to cost. Digital pathology systems that scan biopsy slides to create high-resolution digital images enable remote pathology consultation and computer-aided diagnosis research, though most pathology workflows still involve traditional microscopy. Specialized imaging modalities including narrow-band imaging that enhances visualization of mucosal blood vessel patterns associated with malignancy, and optical coherence tomography providing cross-sectional imaging of tissue microstructure, offer improved diagnostic capabilities but remain confined to research and specialized centers due to cost and complexity.

Telemedicine applications have begun to enable remote specialist consultation in some settings, with primary care providers capturing images of suspicious lesions and transmitting them to distant specialists for assessment. However, these telemedicine programs often lack systematic AI assistance, relying entirely on specialist availability and expertise, and face challenges including image quality variability, lack of standardized capture protocols, and reimbursement barriers. The absence of AI assistance means that telemedicine screening still requires specialist time for each case, limiting scale, and offers no support during the initial examination when the primary provider must decide whether a lesion warrants specialist referral.

Electronic health record systems increasingly include features supporting preventive care including cancer screening, but oral cancer screening capabilities remain limited, typically offering little more than documentation templates for examination findings and referral tracking. Decision support features that might alert providers to risk factors or recommended screening intervals are generally absent or rudimentary. The lack of integration between imaging systems, pathology results, and clinical decision support means that providers must manually synthesize information from multiple sources without computational assistance.

### Limitations and Challenges

The existing system's limitations create substantial barriers to effective oral cancer control and represent the primary motivation for developing AI-augmented alternatives like OralCare AI. Late-stage diagnosis remains endemic, with the majority of oral cancer cases still detected at advanced stages when treatment is more complex, expensive, and less successful. This late detection occurs despite the oral cavity being readily accessible to visual examination, highlighting that access alone is insufficient without accurate interpretation. The shortage of specialist expertise, particularly in developing countries and rural areas, creates geographic inequities where patients' outcomes depend heavily on their location relative to specialist centers. The subjective, experience-dependent nature of visual examination means that diagnostic accuracy varies widely between providers and even within the same provider at different times, reducing consistency and reliability. The slow, multi-step diagnostic pathway from initial detection through biopsy and pathology results creates delays measured in weeks or months during which disease progresses and patient engagement may be lost.

Cost and accessibility barriers prevent many at-risk populations from receiving adequate screening, as dental care is often not covered by health insurance or government programs, and specialist consultations and biopsy procedures carry significant out-of-pocket expenses. Limited awareness among both healthcare providers and the general public about oral cancer risk factors, warning signs, and screening recommendations results in missed screening opportunities and delayed presentation. The lack of systematic screening programs in most settings means that detection relies on opportunistic case-finding during encounters motivated by other concerns, missing patients who do not regularly access dental or medical care despite being at high risk. Documentation and quality monitoring shortcomings make it difficult to assess screening program effectiveness, identify areas for improvement, or conduct research that could enhance detection methods.

These pervasive limitations of the existing system create a compelling case for AI-augmented alternatives that can provide consistent, objective analysis accessible in primary care settings where most patients initially present. By addressing the expertise gap through machine learning models trained on thousands of cases, enhancing consistency through algorithmic assessment unaffected by fatigue or bias, accelerating diagnosis through instant analysis without specialist scheduling, and enabling telemedicine screening in underserved areas, AI-assisted detection has potential to transform oral cancer control from the current suboptimal state to a future where early detection is the norm rather than the exception.

## 2.3 PROPOSED SYSTEM

The proposed OralCare AI system represents a paradigm shift in oral cancer screening, leveraging artificial intelligence to overcome the fundamental limitations of the existing approach while preserving and enhancing the clinical judgment of healthcare providers. Rather than attempting to replace clinicians or existing workflows, the system is designed as an intelligent augmentation tool that provides rapid, consistent, objective analysis of oral lesion images, serving as a second opinion that can increase detection accuracy, accelerate diagnosis, and extend specialist-level screening capabilities to primary care and underserved settings. The proposed system addresses the critical weaknesses identified in the existing system—including expertise gaps, geographic access barriers, diagnostic variability, and workflow inefficiencies—while introducing new capabilities that were previously impossible without AI technology.

### Core Architecture and Capabilities

The proposed OralCare AI system is architected as a modern web application accessible through standard browsers on desktop computers and tablets, eliminating installation and configuration burdens while enabling access from any location with internet connectivity. The system's backend, built using the Django web framework in Python, provides robust foundations including secure user authentication with role-based access control, database management through PostgreSQL hosted on Supabase cloud platform, RESTful API endpoints enabling potential future integrations with electronic health records or other systems, and task scheduling for background processes like model training updates or batch processing. The frontend employs responsive design principles using Bootstrap 5 framework ensuring interfaces adapt elegantly to different screen sizes, HTML5 and CSS3 for modern, accessible markup and styling, JavaScript with jQuery for interactive features and dynamic content updates, and AJAX techniques for smooth, asynchronous data loading without full page refreshes.

The AI inference engine constitutes the technical heart of the system, implementing production-ready versions of two complementary deep learning architectures. RegNetY320, representing the state-of-the-art in efficient convolutional neural network design, serves as the primary detection model, having demonstrated superior accuracy approaching 89.4% on oral cancer classification tasks through systematic architecture optimization that balances model capacity with computational efficiency. VGG16, while representing an older generation of hand-crafted architectures, provides a proven baseline achieving approximately 73.7% accuracy and serves as a validation model whose agreement or disagreement with RegNetY320 predictions provides additional confidence information. Both models accept standardized 224x224 pixel RGB images as input, having been trained on curated datasets of oral lesion photographs encompassing both malignant and benign cases with appropriate augmentation to improve robustness to variations in lighting, angle, and image quality.

The image processing pipeline transforms uploaded clinical photographs into the standardized format required by the AI models through a series of preprocessing steps. Initial validation ensures uploaded files are legitimate image formats and within acceptable size limits, preventing malicious uploads and constraining resource consumption. Color space conversion standardizes images to RGB format if they were captured in other color spaces. Resizing operations employ high-quality resampling algorithms to scale images to the 224x224 pixel dimensions expected by the models while preserving important visual details. Normalization scales pixel intensities to the range expected by the models based on their training, typically zero-centered with unit variance or scaled to [0,1] range. The processed image is then fed to both AI models in parallel, with inference completing in seconds on standard cloud infrastructure, producing probability scores indicating confidence that the lesion is malignant along with binary classifications as either "Cancer" or "Non-Cancer" based on threshold values optimized during model validation.

The data management layer provides secure, scalable storage for all system information including user accounts, image files, detection results, and generated reports. PostgreSQL database hosted on Supabase stores structured data in carefully designed schemas with appropriate relationships, indexes, and constraints ensuring data integrity and query performance. The user table maintains account information including email, hashed password, role, institutional affiliation, and profile details. The image table records uploaded files with metadata including filename, file size, upload timestamp, and reference to the Supabase Storage location. The detection result table stores AI model predictions with confidence scores, processing times, and model version identifiers enabling retrospective analysis. The report table tracks generated PDF documents with patient information and clinical notes. Supabase Storage provides secure file storage for original uploaded images encrypted at rest and in transit, with fine-grained access controls ensuring only authorized users can access specific files, and geographic redundancy protecting against data loss from infrastructure failures.

### Key Features and User Workflows

The multi-role user management system supports diverse stakeholders with tailored interfaces and permissions appropriate to their needs and responsibilities. Administrators have full system access including user account management enabling creation, modification, and deactivation of accounts, system configuration controlling application settings and parameters, usage analytics and monitoring tracking system health and user activity patterns, and database maintenance functions for backup and optimization. Doctor users focus on clinical functionality including patient image upload with drag-and-drop convenience, AI analysis request triggering inference on both models, result review showing confidence scores and classifications, PDF report generation with customizable content, and detection history tracking all analyses for their patients. Researcher users access aggregate analytics including model performance statistics across user populations, usage patterns and adoption metrics, outcome data when available from follow-up, and export capabilities for external analysis. Student users access educational features including practice cases for self-assessment, comparison of student predictions with AI analysis for learning, and supervised access preventing inappropriate clinical use.

The core screening workflow that doctor users typically follow has been designed for maximum efficiency and clinical integration. The workflow begins with image capture during patient examination using an intraoral camera, smartphone with adequate resolution, or DSLR camera with macro lens, following basic guidelines ensuring adequate lighting, focus, and framing to include the entire lesion with surrounding context. The captured image is uploaded to OralCare AI through the intuitive interface where users can drag and drop image files or click to browse and select files, with immediate feedback about upload progress and validation results. Upon successful upload, the system automatically triggers AI analysis on both RegNetY320 and VGG16 models, processing the image and returning results typically within five to ten seconds depending on server load and network latency. The results page presents a comprehensive view including the original uploaded image with zoom capabilities for detailed examination, RegNetY320 prediction with classification label and confidence percentage displayed both numerically and as a visual confidence meter, VGG16 prediction with the same information, consensus indication highlighting agreement or disagreement between models, and clinical guidance suggesting appropriate next steps based on the predictions.

If the clinician decides to generate a formal report based on the analysis, they can access the report generation feature which prompts for patient information including name, age, gender, and medical record number if applicable, clinical notes describing the lesion location and characteristics along with relevant patient history, provider information including the clinician's name and credentials, and report preferences such as inclusion of both model results or just the primary RegNetY320 prediction. The system generates a professionally formatted PDF document incorporating the uploaded image, AI analysis results with appropriate disclaimers, patient and provider information, and timestamp and audit information for record-keeping. The PDF can be downloaded for inclusion in electronic health records, printed for paper charts, or shared with specialists for consultation.

The detection history feature maintains a comprehensive record of all analyses performed, enabling longitudinal tracking of patients with multiple lesions or repeat imaging of the same lesion over time. The history view presents a searchable, filterable table showing upload date and time, patient identifier if recorded, lesion location description, AI predictions from both models, and links to view detailed results or generate reports. This history serves multiple purposes including clinical tracking supporting follow-up on suspicious cases, quality assurance enabling review of past decisions, research data providing datasets for outcome analysis, and audit trail documenting system usage for compliance purposes.

### Technical Innovations and Advantages

The proposed system introduces several technical innovations that represent significant advances over existing approaches. The dual-model architecture providing both RegNetY320 and VGG16 predictions offers built-in validation through consensus analysis, with agreement between models increasing confidence in predictions while disagreement flagging cases warranting additional scrutiny or specialist consultation. This approach addresses a key limitation of single-model systems where failures can occur silently without detection. The comparative implementation also enables ongoing evaluation of different architectural approaches, contributing to research understanding of optimal deep learning strategies for medical image classification.

The cloud-native architecture leveraging modern platforms including Django for application logic, PostgreSQL for structured data, and Supabase for cloud services provides inherent scalability where additional users and data can be accommodated through infrastructure scaling without application redesign, reliability through automated backups and redundancy, security through encryption and access controls meeting healthcare privacy requirements, and cost-efficiency through pay-per-use pricing eliminating overprovisioning waste. This architecture enables deployment scenarios ranging from small pilot studies with dozens of users to large screening programs serving thousands of providers across entire regions or countries.

The rapid inference capability with results available within seconds of image upload represents a critical improvement over specialist consultation workflows that involve appointment scheduling, wait times, and communication delays. This near-instantaneous feedback enables point-of-care decision-making where providers receive AI analysis during the same patient encounter, allowing immediate discussion of results and next steps. The speed advantage is achieved through model optimization including quantization reducing numerical precision without significant accuracy loss, TensorRT optimization or similar techniques compiling models for efficient execution, GPU acceleration utilizing specialized hardware for parallel computation, and caching strategies reducing redundant computation.

The comprehensive reporting features that generate professional PDF documents suitable for medical records address a critical requirement for clinical adoption. Many research AI systems demonstrate impressive accuracy but lack the documentation and workflow integration necessary for practical use. OralCare AI treats reporting as a first-class feature, with careful attention to clinical information requirements, medical-legal documentation standards, professional formatting and presentation, and integration with existing record-keeping practices. The reports serve not only as documentation but also as patient education tools, with visual presentations of AI findings that providers can use to explain diagnosis and recommended next steps.

The role-based access control system enabling different functionality and data access for administrators, doctors, researchers, and students reflects understanding that healthcare AI serves diverse stakeholders. This multi-stakeholder design enables OralCare AI to support not only clinical screening but also education of future healthcare providers, research into screening effectiveness and optimal implementation strategies, and administrative oversight ensuring appropriate system use and performance. The separation of concerns between roles also enhances security by limiting each user's access to only the data and functions necessary for their legitimate purposes, following principle of least privilege.

### Benefits and Impact

The proposed OralCare AI system offers substantial benefits across multiple dimensions compared to the existing approach. Clinical benefits include improved detection accuracy through AI assistance, particularly for general practitioners who may lack specialized expertise in oral pathology, reduced diagnostic variability by providing consistent, objective analysis unaffected by fatigue or cognitive biases, accelerated triage enabling rapid identification of cases requiring specialist referral versus those suitable for observation, and enhanced specialist consultation through structured image and data sharing when referral is warranted. Operational benefits encompass reduced costs from avoiding unnecessary biopsies of benign lesions flagged by overly cautious visual examination, workflow efficiency through rapid analysis fitting naturally into clinical encounters, documentation automation reducing administrative burden, and telemedicine enablement extending capabilities to remote areas.

Educational benefits include training support for medical and dental students who can practice diagnostic skills with immediate AI feedback, continuing education for practicing providers exposing them to diverse case presentations, and standardization of screening approaches through guideline implementation in software. Research benefits involve data aggregation creating longitudinal databases for epidemiological analysis, outcome tracking enabling assessment of AI impact on early detection rates, algorithm improvement through continuous learning from new cases, and evidence generation supporting AI adoption through demonstration of clinical utility. Societal benefits ultimately include improved health outcomes through earlier cancer detection, reduced healthcare costs from preventing late-stage disease, expanded access bringing screening to underserved populations, and advancement of medical AI field through successful deployment example.

The proposed system represents a comprehensive, thoughtfully designed solution that addresses real clinical needs while demonstrating technical feasibility, operational practicality, and potential for meaningful impact on oral cancer outcomes. The following chapters detail the specific implementation approach, testing methodology, and validation results demonstrating that the system achieves its objectives.

